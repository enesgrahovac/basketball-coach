{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basketball Shot Analysis - Model Comparison & Evaluation\n",
        "\n",
        "This notebook compares different models, prompts, and configurations for basketball shot analysis.\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "- **Accuracy**: Make/miss and shot type classification accuracy\n",
        "- **Latency**: Average inference time per sample\n",
        "- **Cost**: Estimated cost per API call\n",
        "- **Reliability**: Parse success rate and error analysis\n",
        "\n",
        "## Models Tested\n",
        "\n",
        "- Gemini 2.5 Flash (production)\n",
        "- Gemini 2.5 Flash Lite (fast)\n",
        "- Gemini 2.5 Pro (high quality)\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Ensure data is synced (run `1_data_sync.ipynb` first)\n",
        "2. Set your API keys in environment variables\n",
        "3. Configure evaluation parameters below\n",
        "4. Run evaluation and analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add src directory to path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "from data_manager import EvaluationDataManager\n",
        "from evaluator import ModelEvaluator\n",
        "from metrics import EvaluationMetrics\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('../../.env')\n",
        "\n",
        "# Configuration\n",
        "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
        "TEST_MODE = True  # Set to False for full evaluation\n",
        "MAX_SAMPLES = 5 if TEST_MODE else None  # Limit samples for testing\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    print(\"‚ö†Ô∏è  Missing GEMINI_API_KEY!\")\n",
        "    print(\"Please set your Gemini API key in the .env file\")\n",
        "else:\n",
        "    print(\"‚úÖ API key loaded\")\n",
        "\n",
        "print(f\"üîß Test mode: {TEST_MODE}\")\n",
        "if TEST_MODE:\n",
        "    print(f\"üìä Will evaluate on {MAX_SAMPLES} samples only\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configurations\n",
        "with open('../configs/models.yaml', 'r') as f:\n",
        "    model_config = yaml.safe_load(f)\n",
        "\n",
        "with open('../configs/prompts.yaml', 'r') as f:\n",
        "    prompt_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"üìã Available Models:\")\n",
        "for model_key, model_info in model_config['models'].items():\n",
        "    print(f\"  {model_key}: {model_info['name']} - {model_info['description']}\")\n",
        "\n",
        "print(\"\\nüìù Available Prompts:\")\n",
        "for prompt_key, prompt_info in prompt_config['prompts'].items():\n",
        "    print(f\"  {prompt_key}: {prompt_info['name']} - {prompt_info['description']}\")\n",
        "\n",
        "# Select models and prompts to evaluate\n",
        "MODELS_TO_TEST = ['gemini_flash', 'gemini_flash_lite']  # Add 'gemini_pro' for full eval\n",
        "PROMPTS_TO_TEST = ['current_production', 'simplified']  # Add more for comprehensive testing\n",
        "VIDEO_CONFIG = 'standard'  # standard, high_quality, or fast\n",
        "\n",
        "print(f\"\\nüéØ Evaluation Plan:\")\n",
        "print(f\"Models: {MODELS_TO_TEST}\")\n",
        "print(f\"Prompts: {PROMPTS_TO_TEST}\")\n",
        "print(f\"Video config: {VIDEO_CONFIG}\")\n",
        "print(f\"Total combinations: {len(MODELS_TO_TEST) * len(PROMPTS_TO_TEST)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load ground truth data\n",
        "data_manager = EvaluationDataManager(\n",
        "    supabase_url=\"dummy\",  # Not needed for loading\n",
        "    supabase_key=\"dummy\",\n",
        "    data_dir='../data'\n",
        ")\n",
        "\n",
        "try:\n",
        "    ground_truth = data_manager.load_ground_truth()\n",
        "    print(f\"‚úÖ Loaded {len(ground_truth)} ground truth samples\")\n",
        "    \n",
        "    # Show dataset overview\n",
        "    shot_types = [entry['ground_truth']['shot_type'] for entry in ground_truth]\n",
        "    results = [entry['ground_truth']['result'] for entry in ground_truth]\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"Shot types: {pd.Series(shot_types).value_counts().to_dict()}\")\n",
        "    print(f\"Results: {pd.Series(results).value_counts().to_dict()}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Ground truth data not found!\")\n",
        "    print(\"Please run the data synchronization notebook first (1_data_sync.ipynb)\")\n",
        "    ground_truth = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator(\n",
        "    gemini_api_key=GEMINI_API_KEY,\n",
        "    output_dir='../data/model_outputs'\n",
        ")\n",
        "\n",
        "# Run evaluations\n",
        "evaluation_results = []\n",
        "\n",
        "if ground_truth:\n",
        "    video_settings = model_config['video_configs'][VIDEO_CONFIG]\n",
        "    \n",
        "    for model_key in MODELS_TO_TEST:\n",
        "        model_info = model_config['models'][model_key]\n",
        "        model_name = model_info['name']\n",
        "        \n",
        "        for prompt_key in PROMPTS_TO_TEST:\n",
        "            prompt_info = prompt_config['prompts'][prompt_key]\n",
        "            prompt_text = prompt_info['content']\n",
        "            \n",
        "            print(f\"\\nüöÄ Evaluating {model_key} with {prompt_key} prompt...\")\n",
        "            print(f\"Model: {model_name}\")\n",
        "            print(f\"Video: {video_settings['fps']}fps, {video_settings['resolution']} resolution\")\n",
        "            \n",
        "            try:\n",
        "                result = evaluator.evaluate_model(\n",
        "                    model_name=model_name,\n",
        "                    prompt=prompt_text,\n",
        "                    ground_truth_data=ground_truth,\n",
        "                    fps=video_settings['fps'],\n",
        "                    media_resolution=video_settings['resolution'],\n",
        "                    max_samples=MAX_SAMPLES\n",
        "                )\n",
        "                \n",
        "                # Add configuration info\n",
        "                result['model_key'] = model_key\n",
        "                result['prompt_key'] = prompt_key\n",
        "                result['video_config'] = VIDEO_CONFIG\n",
        "                \n",
        "                evaluation_results.append(result)\n",
        "                \n",
        "                print(f\"‚úÖ Completed {model_key} + {prompt_key}\")\n",
        "                print(f\"   Accuracy: {result['metrics']['both_correct_accuracy']:.3f}\")\n",
        "                print(f\"   Avg time: {result['average_inference_time_s']:.2f}s\")\n",
        "                print(f\"   Total cost: ${result['total_cost_usd']:.4f}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error evaluating {model_key} + {prompt_key}: {e}\")\n",
        "                continue\n",
        "\n",
        "print(f\"\\nüèÅ Evaluation complete! {len(evaluation_results)} combinations tested.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "if evaluation_results:\n",
        "    comparison_data = []\n",
        "    \n",
        "    for result in evaluation_results:\n",
        "        row = {\n",
        "            'model': result['model_key'],\n",
        "            'prompt': result['prompt_key'],\n",
        "            'model_name': result['model_name'],\n",
        "            'total_samples': result['total_samples'],\n",
        "            'shot_type_accuracy': result['metrics']['shot_type_accuracy'],\n",
        "            'result_accuracy': result['metrics']['result_accuracy'],\n",
        "            'both_correct_accuracy': result['metrics']['both_correct_accuracy'],\n",
        "            'parse_success_rate': result['metrics']['parse_success_rate'],\n",
        "            'average_confidence': result['metrics']['average_confidence'],\n",
        "            'avg_inference_time_s': result['average_inference_time_s'],\n",
        "            'total_cost_usd': result['total_cost_usd'],\n",
        "            'cost_per_sample': result['total_cost_usd'] / result['total_samples'] if result['total_samples'] > 0 else 0\n",
        "        }\n",
        "        comparison_data.append(row)\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"üìä Evaluation Results Summary:\")\n",
        "    print(comparison_df[['model', 'prompt', 'both_correct_accuracy', 'avg_inference_time_s', 'cost_per_sample']].round(4))\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No evaluation results to display\")\n",
        "    comparison_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed analysis and visualizations\n",
        "if comparison_df is not None and len(comparison_df) > 0:\n",
        "    \n",
        "    # Performance comparison chart\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Accuracy comparison\n",
        "    ax1 = axes[0, 0]\n",
        "    x_pos = np.arange(len(comparison_df))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax1.bar(x_pos - width/2, comparison_df['shot_type_accuracy'], width, label='Shot Type', alpha=0.8)\n",
        "    ax1.bar(x_pos + width/2, comparison_df['result_accuracy'], width, label='Make/Miss', alpha=0.8)\n",
        "    \n",
        "    ax1.set_xlabel('Model + Prompt')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.set_title('Classification Accuracy Comparison')\n",
        "    ax1.set_xticks(x_pos)\n",
        "    ax1.set_xticklabels([f\"{row['model']}\\n{row['prompt']}\" for _, row in comparison_df.iterrows()], rotation=45)\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Speed vs Accuracy\n",
        "    ax2 = axes[0, 1]\n",
        "    scatter = ax2.scatter(comparison_df['avg_inference_time_s'], comparison_df['both_correct_accuracy'], \n",
        "                         s=100, alpha=0.7, c=comparison_df['cost_per_sample'], cmap='viridis')\n",
        "    \n",
        "    for i, row in comparison_df.iterrows():\n",
        "        ax2.annotate(f\"{row['model'][:8]}\\n{row['prompt'][:8]}\", \n",
        "                    (row['avg_inference_time_s'], row['both_correct_accuracy']),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "    \n",
        "    ax2.set_xlabel('Average Inference Time (s)')\n",
        "    ax2.set_ylabel('Both Correct Accuracy')\n",
        "    ax2.set_title('Speed vs Accuracy (Color = Cost)')\n",
        "    plt.colorbar(scatter, ax=ax2, label='Cost per Sample ($)')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Cost analysis\n",
        "    ax3 = axes[1, 0]\n",
        "    bars = ax3.bar(range(len(comparison_df)), comparison_df['cost_per_sample'])\n",
        "    ax3.set_xlabel('Model + Prompt')\n",
        "    ax3.set_ylabel('Cost per Sample ($)')\n",
        "    ax3.set_title('Cost per Sample Comparison')\n",
        "    ax3.set_xticks(range(len(comparison_df)))\n",
        "    ax3.set_xticklabels([f\"{row['model']}\\n{row['prompt']}\" for _, row in comparison_df.iterrows()], rotation=45)\n",
        "    \n",
        "    # Add accuracy as text on bars\n",
        "    for i, (bar, acc) in enumerate(zip(bars, comparison_df['both_correct_accuracy'])):\n",
        "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(comparison_df['cost_per_sample'])*0.01,\n",
        "                f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Overall performance radar (if multiple models)\n",
        "    ax4 = axes[1, 1]\n",
        "    if len(comparison_df) > 1:\n",
        "        # Normalize metrics for radar chart\n",
        "        metrics_to_plot = ['both_correct_accuracy', 'parse_success_rate', 'average_confidence']\n",
        "        normalized_data = comparison_df[metrics_to_plot].copy()\n",
        "        \n",
        "        for col in metrics_to_plot:\n",
        "            max_val = normalized_data[col].max()\n",
        "            if max_val > 0:\n",
        "                normalized_data[col] = normalized_data[col] / max_val\n",
        "        \n",
        "        # Simple bar chart instead of radar for simplicity\n",
        "        x_pos = np.arange(len(metrics_to_plot))\n",
        "        width = 0.8 / len(comparison_df)\n",
        "        \n",
        "        for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
        "            values = [normalized_data.iloc[i][col] for col in metrics_to_plot]\n",
        "            ax4.bar(x_pos + i * width, values, width, \n",
        "                   label=f\"{row['model']} + {row['prompt']}\", alpha=0.7)\n",
        "        \n",
        "        ax4.set_xlabel('Metrics')\n",
        "        ax4.set_ylabel('Normalized Score')\n",
        "        ax4.set_title('Overall Performance Comparison')\n",
        "        ax4.set_xticks(x_pos + width * (len(comparison_df) - 1) / 2)\n",
        "        ax4.set_xticklabels(['Both Correct', 'Parse Success', 'Avg Confidence'])\n",
        "        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Need multiple models\\nfor comparison', \n",
        "                ha='center', va='center', transform=ax4.transAxes, fontsize=12)\n",
        "        ax4.set_title('Overall Performance Comparison')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available for visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed error analysis\n",
        "if evaluation_results:\n",
        "    metrics_calculator = EvaluationMetrics()\n",
        "    \n",
        "    print(\"üîç Detailed Error Analysis:\")\n",
        "    \n",
        "    for i, result in enumerate(evaluation_results):\n",
        "        model_key = result['model_key']\n",
        "        prompt_key = result['prompt_key']\n",
        "        \n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üìä {model_key.upper()} + {prompt_key.upper()}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        \n",
        "        metrics = result['metrics']\n",
        "        \n",
        "        # Overall performance\n",
        "        print(f\"üìà Performance Summary:\")\n",
        "        print(f\"  Total samples: {result['total_samples']}\")\n",
        "        print(f\"  Parse success: {metrics['parse_success_rate']:.3f}\")\n",
        "        print(f\"  Shot type accuracy: {metrics['shot_type_accuracy']:.3f}\")\n",
        "        print(f\"  Make/miss accuracy: {metrics['result_accuracy']:.3f}\")\n",
        "        print(f\"  Both correct: {metrics['both_correct_accuracy']:.3f}\")\n",
        "        print(f\"  Average confidence: {metrics['average_confidence']:.3f}\")\n",
        "        print(f\"  Avg inference time: {result['average_inference_time_s']:.2f}s\")\n",
        "        print(f\"  Cost per sample: ${result['total_cost_usd']/result['total_samples']:.4f}\")\n",
        "        \n",
        "        # Confusion matrices\n",
        "        print(f\"\\nüéØ Shot Type Confusion:\")\n",
        "        shot_type_cm = metrics.get('shot_type_confusion_matrix', {})\n",
        "        for transition, count in sorted(shot_type_cm.items()):\n",
        "            if count > 0:\n",
        "                print(f\"  {transition}: {count}\")\n",
        "        \n",
        "        print(f\"\\nüéØ Make/Miss Confusion:\")\n",
        "        result_cm = metrics.get('result_confusion_matrix', {})\n",
        "        for transition, count in sorted(result_cm.items()):\n",
        "            if count > 0:\n",
        "                print(f\"  {transition}: {count}\")\n",
        "        \n",
        "        # Error analysis\n",
        "        error_df = metrics_calculator.generate_error_analysis(result)\n",
        "        if len(error_df) > 0:\n",
        "            print(f\"\\n‚ùå Error Breakdown ({len(error_df)} errors):\")\n",
        "            print(f\"  Shot type errors: {error_df['shot_type_error'].sum()}\")\n",
        "            print(f\"  Make/miss errors: {error_df['result_error'].sum()}\")\n",
        "            print(f\"  Parse errors: {error_df['has_parse_error'].sum()}\")\n",
        "            \n",
        "            if len(error_df) > 0:\n",
        "                print(f\"\\nüîç Sample Errors:\")\n",
        "                for _, row in error_df.head(3).iterrows():\n",
        "                    print(f\"  Clip {row['clip_id'][:8]}:\")\n",
        "                    if row['shot_type_error']:\n",
        "                        print(f\"    Shot type: {row['gt_shot_type']} ‚Üí {row['pred_shot_type']}\")\n",
        "                    if row['result_error']:\n",
        "                        print(f\"    Result: {row['gt_result']} ‚Üí {row['pred_result']}\")\n",
        "                    print(f\"    Confidence: {row['confidence']:.3f}\")\n",
        "        else:\n",
        "            print(f\"\\n‚úÖ No errors found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recommendations and next steps\n",
        "if comparison_df is not None and len(comparison_df) > 0:\n",
        "    print(\"üéØ RECOMMENDATIONS\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Find best performers\n",
        "    best_accuracy = comparison_df.loc[comparison_df['both_correct_accuracy'].idxmax()]\n",
        "    fastest = comparison_df.loc[comparison_df['avg_inference_time_s'].idxmin()]\n",
        "    cheapest = comparison_df.loc[comparison_df['cost_per_sample'].idxmin()]\n",
        "    \n",
        "    print(f\"üèÜ BEST ACCURACY: {best_accuracy['model']} + {best_accuracy['prompt']}\")\n",
        "    print(f\"   Accuracy: {best_accuracy['both_correct_accuracy']:.3f}\")\n",
        "    print(f\"   Speed: {best_accuracy['avg_inference_time_s']:.2f}s\")\n",
        "    print(f\"   Cost: ${best_accuracy['cost_per_sample']:.4f}\")\n",
        "    \n",
        "    print(f\"\\n‚ö° FASTEST: {fastest['model']} + {fastest['prompt']}\")\n",
        "    print(f\"   Speed: {fastest['avg_inference_time_s']:.2f}s\")\n",
        "    print(f\"   Accuracy: {fastest['both_correct_accuracy']:.3f}\")\n",
        "    print(f\"   Cost: ${fastest['cost_per_sample']:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüí∞ CHEAPEST: {cheapest['model']} + {cheapest['prompt']}\")\n",
        "    print(f\"   Cost: ${cheapest['cost_per_sample']:.4f}\")\n",
        "    print(f\"   Accuracy: {cheapest['both_correct_accuracy']:.3f}\")\n",
        "    print(f\"   Speed: {cheapest['avg_inference_time_s']:.2f}s\")\n",
        "    \n",
        "    # Production recommendation\n",
        "    print(f\"\\nüöÄ PRODUCTION RECOMMENDATION:\")\n",
        "    \n",
        "    # Calculate a balanced score (accuracy * 0.6 + speed_score * 0.2 + cost_score * 0.2)\n",
        "    comparison_df_scored = comparison_df.copy()\n",
        "    \n",
        "    # Normalize scores (higher is better)\n",
        "    comparison_df_scored['speed_score'] = 1 / comparison_df_scored['avg_inference_time_s']\n",
        "    comparison_df_scored['cost_score'] = 1 / comparison_df_scored['cost_per_sample']\n",
        "    \n",
        "    # Normalize to 0-1 range\n",
        "    comparison_df_scored['speed_score'] = (comparison_df_scored['speed_score'] - comparison_df_scored['speed_score'].min()) / (comparison_df_scored['speed_score'].max() - comparison_df_scored['speed_score'].min())\n",
        "    comparison_df_scored['cost_score'] = (comparison_df_scored['cost_score'] - comparison_df_scored['cost_score'].min()) / (comparison_df_scored['cost_score'].max() - comparison_df_scored['cost_score'].min())\n",
        "    \n",
        "    # Combined score\n",
        "    comparison_df_scored['combined_score'] = (\n",
        "        comparison_df_scored['both_correct_accuracy'] * 0.6 +\n",
        "        comparison_df_scored['speed_score'] * 0.2 +\n",
        "        comparison_df_scored['cost_score'] * 0.2\n",
        "    )\n",
        "    \n",
        "    best_overall = comparison_df_scored.loc[comparison_df_scored['combined_score'].idxmax()]\n",
        "    \n",
        "    print(f\"   Best balanced option: {best_overall['model']} + {best_overall['prompt']}\")\n",
        "    print(f\"   Combined score: {best_overall['combined_score']:.3f}\")\n",
        "    print(f\"   Accuracy: {best_overall['both_correct_accuracy']:.3f}\")\n",
        "    print(f\"   Speed: {best_overall['avg_inference_time_s']:.2f}s\") \n",
        "    print(f\"   Cost: ${best_overall['cost_per_sample']:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüìã NEXT STEPS:\")\n",
        "    print(f\"1. Run full evaluation (set TEST_MODE=False) on complete dataset\")\n",
        "    print(f\"2. Test additional models: Gemini Pro, other providers\")\n",
        "    print(f\"3. Experiment with prompt variations and video settings\")\n",
        "    print(f\"4. Implement A/B testing in production\")\n",
        "    print(f\"5. Set up continuous evaluation pipeline\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No results available for recommendations\")\n",
        "\n",
        "print(f\"\\n‚úÖ Model comparison complete!\")\n",
        "print(f\"üìÅ Results saved to: ../data/model_outputs/\")\n",
        "print(f\"üìä Run this notebook again with TEST_MODE=False for full evaluation\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
